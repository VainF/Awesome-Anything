# Awesome-Anything
[![Awesome Anything](https://img.shields.io/badge/Awesome-Anything-blue)](https://github.com/topics/awesome)

A curated list of **general AI methods for Anything**: AnyObject, AnyGeneration, AnyModel, AnyTask, etc.

[Contributions](https://github.com/VainF/Awesome-Anything/pulls) are welcome!

- [Awesome-Anything](#awesome-anything)
  - [AnyObject](#anyobject)
  - [AnyGeneration](#anygeneration)
  - [AnyModel](#anymodel)
  - [AnyTask](#anytask)
  
## AnyObject

| Title & Authors | Intro | Useful Links |
|:----|  :----: | :---:|
| [**Segment Anything**](https://arxiv.org/abs/2304.02643) <br> *Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alex Berg, Wan-Yen Lo, Piotr Dollar, Ross Girshick* <br> Preprint'23 <br><br> [[**Segment Anything (Project)**](https://github.com/facebookresearch/segment-anything)] | ![intro](https://github.com/facebookresearch/segment-anything/blob/main/assets/masks2.jpg?raw=true) | [[Github](https://github.com/facebookresearch/segment-anything)] <br> [[Page](https://segment-anything.com/)] <br> [[Demo](https://segment-anything.com/demo)] |
|  [**Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection**](https://arxiv.org/abs/2303.05499) <br> *Shilong Liu and Zhaoyang Zeng and Tianhe Ren and Feng Li and Hao Zhang and Jie Yang and Chunyuan Li and Jianwei Yang and Hang Su and Jun Zhu and Lei Zhang* <br> Preprint'23 <br><br> [[**Grounded-SAM**](https://github.com/IDEA-Research/Grounded-Segment-Anything), [**GroundingDINO (Project)**](https://github.com/IDEA-Research/GroundingDINO)] | ![intro](https://github.com/IDEA-Research/Grounded-Segment-Anything/raw/main/assets/grounded_sam_demo3_demo4.png) | [[Github](https://github.com/IDEA-Research/Grounded-Segment-Anything)] <br> [[Demo](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb)] |
| [**SegGPT: Segmenting Everything In Context**](https://arxiv.org/abs/2304.03284) <br> *Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, Tiejun Huang* <br> Preprint'23 <br><br>[[**SegGPT (Project)**](https://github.com/baaivision/Painter)] | <img width="903" alt="image" src="https://user-images.githubusercontent.com/18592211/230897227-c797f375-a44d-4536-a06b-41f0d9f4dbc4.png"> | [[Github](https://github.com/baaivision/Painter)] |
| [**V3Det: Vast Vocabulary Visual Detection Dataset**](https://arxiv.org/abs/2304.03752) <br> *Jiaqi Wang, Pan Zhang, Tao Chu, Yuhang Cao, Yujie Zhou, Tong Wu, Bin Wang, Conghui He, Dahua Lin* <br> Preprint'23  | ![image](https://user-images.githubusercontent.com/18592211/230936730-4837c3ea-1af5-470c-8532-d0d7bd245df7.png) | -- |
| [**segment-anything-video (Project)**](https://github.com/kadirnar/segment-anything-video) <br> Kadir Nar | ![intro](https://github.com/kadirnar/segment-anything-pip/releases/download/v0.2.2/metaseg_demo.gif)  | [[Github](https://github.com/kadirnar/segment-anything-video)] |
| [**Towards Segmenting Anything That Moves**](https://arxiv.org/abs/1902.03715) <br> *Achal Dave, Pavel Tokmakov, Deva Ramanan* <br> ICCV'19 Workshop <br><br> [[**segment-any-moving (Project)**](https://github.com/achalddave/segment-any-moving)] | [<img src="http://www.achaldave.com/projects/anything-that-moves/videos/ZXN6A-tracked-with-objectness-trimmed.gif" width="32%" />](http://www.achaldave.com/projects/anything-that-moves/videos/ZXN6A-tracked-with-objectness-trimmed.mp4)[<img src="http://www.achaldave.com/projects/anything-that-moves/videos/c95cd17749.gif" width="32%" />](http://www.achaldave.com/projects/anything-that-moves/videos/c95cd17749.mp4)<img src="http://www.achaldave.com/projects/anything-that-moves/videos/e0bdb5dfae.gif" width="32%" /> | [[Github](https://github.com/achalddave/segment-any-moving)] |
| [**Semantic Segment Anything**](https://github.com/fudan-zvg/Semantic-Segment-Anything) <br> *Jiaqi Chen, Zeyu Yang, Li Zhang* <br><br> [[**Semantic-Segment-Anything (Project)**](https://github.com/fudan-zvg/Semantic-Segment-Anything)] | <img width="903" alt="image" src="https://github.com/fudan-zvg/Semantic-Segment-Anything/blob/main/figures/SSA_motivation.png"> | [[Github](https://github.com/fudan-zvg/Semantic-Segment-Anything)] |
| [**Segment Anything and Name It (Project)**](https://github.com/Cheems-Seminar/segment-anything-and-name-it) <br> *Peize Sun* and *Shoufa Chen* | ![intro](https://github.com/Cheems-Seminar/segment-anything-and-name-it/raw/main/assets/teaser.png) | [[Github](https://github.com/Cheems-Seminar/segment-anything-and-name-it)]
| [**GroundedSAM-zero-shot-anomaly-detection (Project)**](https://github.com/caoyunkang/GroundedSAM-zero-shot-anomaly-detection) <br> *Yunkang Cao*  | <img width="677" alt="image" src="https://user-images.githubusercontent.com/18592211/231068964-ddeae0ea-4e83-40d6-b73e-2811d46f808d.png"> | [[Github](https://github.com/caoyunkang/GroundedSAM-zero-shot-anomaly-detection)] |
| [**Segment Anything Labelling Tool (SALT) (Project)**](https://github.com/anuragxel/salt) <br> *Anurag Ghosh*  | ![intro](https://github.com/anuragxel/salt/raw/main/assets/how-it-works.gif) | [[Github](https://github.com/caoyunkang/GroundedSAM-zero-shot-anomaly-detection)] |
| [**Prompt-Segment-Anything (Project)**](https://github.com/RockeyCoss/Prompt-Segment-Anything) <br> *Anurag Ghosh*  | ![intro](https://github.com/RockeyCoss/Prompt-Segment-Anything/raw/master/assets/example1.jpg) | [[Github](https://github.com/RockeyCoss/Prompt-Segment-Anything)]|
| [**SAM-RBox (Project)**](https://github.com/Li-Qingyun/sam-mmrotate) <br> *Qingyun Li*  | ![intro](https://user-images.githubusercontent.com/79644233/230732578-649086b4-7720-4450-9e87-25873bec07cb.png) | [[Github](https://github.com/Li-Qingyun/sam-mmrotate)] |
|  [**VISAM**]() <br> *Feng Yan, Weixin Luo* <br> Preprint'23 <br><br> [[**MOT+SAM**]() | ![intro](https://raw.githubusercontent.com/BingfengYan/MOTSAM/main/tmp.gif) | [[Github](https://github.com/BingfengYan/VISAM)] <br>|

## AnyGeneration
| Title & Authors | Intro | Useful Links |
|:----|  :----: | :---:|
| [**High-Resolution Image Synthesis with Latent Diffusion Models**](https://arxiv.org/abs/2112.10752) <br> *Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Bj√∂rn Ommer* <br> CVPR'22 <br><br> [[**Stable-Diffusion (Project)**](https://github.com/CompVis/stable-diffusion)] | ![intro](https://r2.stablediffusionweb.com/images/stable-diffusion-demo-2.webp) | [[Github](https://github.com/CompVis/stable-diffusion)] <br> [[Page](https://stablediffusionweb.com/)] <br> [[Demo](https://stablediffusionweb.com/#demo)] |
| [**Adding Conditional Control to Text-to-Image Diffusion Models**](https://arxiv.org/abs/2302.05543) <br> *Lvmin Zhang, Maneesh Agrawala* <br> Preprint'23 <br><br> [[**ControlNet (Project)**](https://github.com/lllyasviel/ControlNet)] | ![intro](https://huggingface.co/datasets/YiYiXu/test-doc-assets/resolve/main/blog_post_cell_16_output_1.jpeg) | [[Github](https://github.com/lllyasviel/ControlNet)] <br>  [[Demo](https://huggingface.co/spaces/hysts/ControlNet)] |
| [**GigaGAN: Large-scale GAN for Text-to-Image Synthesis**](https://arxiv.org/abs/2303.05511) <br> *Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, Taesung Park* <br> CVPR'23 | <img alt="image" src="https://user-images.githubusercontent.com/18592211/230898538-84da51ee-f686-422d-9892-c1c47ab10b75.png"></img> | [[Page](https://mingukkang.github.io/GigaGAN/)] |
| [**Inpaint-Anything: Segment Anything Meets Image Inpainting (Project)**](https://github.com/geekyutao/Inpaint-Anything) <br> *Tao Yu*  | ![intro](https://github.com/geekyutao/Inpaint-Anything/raw/main/example/framework.png) | [[Github](https://github.com/geekyutao/Inpaint-Anything)] |
| [**IEA: Image Editing Anything (Project)**](https://github.com/feizc/IEA) <br> *Zhengcong Fei*  | ![intro](https://user-images.githubusercontent.com/37614046/230707537-206c0714-de32-41cd-a277-203fd57cd300.png) | [[Github](https://github.com/feizc/IEA)] |
| [**EditAnything (Project)**](https://github.com/sail-sg/EditAnything) <br> *Shanghua Gao, Pan Zhou*  | ![intro](https://github.com/sail-sg/EditAnything/raw/main/images/edit_sample1.jpg) | [[Github](https://github.com/sail-sg/EditAnything)] |


## AnyModel
| Title & Authors | Intro | Useful Links |
|:----|  :----: | :---:|
| [**HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace**](https://arxiv.org/abs/2303.17580) <br> *Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang* <br> Preprint'23 <br><br> [[**Jarvis (Project)**](https://github.com/microsoft/JARVIS)] |  <img  src="https://github.com/microsoft/JARVIS/raw/main/assets/overview.jpg"><img> | [[Github](https://github.com/microsoft/JARVIS)] <br> [[Demo](https://huggingface.co/spaces/microsoft/HuggingGPT)] |
| [**TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs**](https://arxiv.org/abs/2303.16434) <br> *Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, Yun Wang, Linjun Shou, Ming Gong, Nan Duan* Preprint'23 | ![intro](https://github.com/microsoft/visual-chatgpt/raw/main/assets/overview.png) | [[Github](https://github.com/microsoft/visual-chatgpt/tree/main/TaskMatrix.AI)] |
| [**DepGraph: Towards Any Structural Pruning**](https://arxiv.org/abs/2301.12900) <br> *Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi Mi, Xinchao Wang* <br> CVPR'23 <br><br> [[**Torch-Pruning (Project)**](https://github.com/VainF/Torch-Pruning)] | ![intro](https://github.com/VainF/Torch-Pruning/raw/master/assets/intro.png) | [[Github](https://github.com/VainF/Torch-Pruning)] <br> [[Demo](https://colab.research.google.com/drive/1TRvELQDNj9PwM-EERWbF3IQOyxZeDepp?usp=sharing)] |
| [**MQBench: Towards Reproducible and Deployable Model Quantization Benchmark**](https://arxiv.org/abs/2111.03759) <br> *Yuhang Li and Mingzhu Shen and Jian Ma and Yan Ren and Mingxin Zhao and Qi Zhang and Ruihao Gong and Fengwei Yu and Junjie Yan* <br> NeurIPS'21 <br><br> [[**MQBench (Project)**](https://github.com/ModelTC/MQBench)] | ![intro](http://mqbench.tech/assets/img/overview.png) | [[Github](https://github.com/ModelTC/MQBench)] <br> [[Page](http://mqbench.tech/)] |
| [**OTOv2: Automatic, Generic, User-Friendly**](https://openreview.net/pdf?id=7ynoX1ojPMt) <br> *Tianyi Chen, Luming Liang, Tianyu Ding, Ilya Zharkov* <br> ICLR'23 <br><br> [[**Only Train Once (Project)**](https://github.com/tianyic/only_train_once)] | ![intro](https://user-images.githubusercontent.com/8930611/230513048-e07b09a2-b29b-49ad-a47f-52630337ab2a.png) | [[Github](https://github.com/tianyic/only_train_once)] |
| [**Deep Model Reassembly**](https://arxiv.org/abs/2210.17409) <br> *Xingyi Yang, Daquan Zhou, Songhua Liu, Jingwen Ye, Xinchao Wang* <br> NeurIPS'22 <br><br> [[**Deep Model Reassembly (Project)**](https://github.com/Adamdad/DeRy)] <br>  | ![intro](https://github.com/Adamdad/DeRy/raw/main/assets/pipeline.png) | [[Github](https://github.com/Adamdad/DeRy)] <br> [[Page](https://adamdad.github.io/dery/)] |
| [**OpenAGI: When LLM Meets Domain Experts**](https://arxiv.org/pdf/2304.04370.pdf) <br> *Yingqiang Ge, Wenyue Hua, Jianchao Ji, Juntao Tan, Shuyuan Xu, Yongfeng Zhang* <br><br> [[**OpenAGI (Project)**](https://github.com/agiresearch/OpenAGI)] | ![intro](https://github.com/agiresearch/OpenAGI/raw/main/image/pipeline.png) | [Github](https://github.com/agiresearch/OpenAGI) |

## AnyTask
| Title & Authors | Intro | Useful Links |
|:----|  :----: | :---:|
| [**Generalized Decoding for Pixel, Image and Language**](https://arxiv.org/abs/2212.11270) <br> *Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, Lu Yuan, Nanyun Peng, Lijuan Wang, Yong Jae Lee, Jianfeng Gao* <br> CVPR'23 <br><br> [[**X-Decoder (Project)**](https://github.com/microsoft/X-Decoder/)] | ![intro](https://user-images.githubusercontent.com/11957155/210801832-c9143c42-ef65-4501-95a5-0d54749dcc52.gif)  | [[Github](https://github.com/microsoft/X-Decoder/)] <br> [[Page](https://x-decoder-vl.github.io)] <br> [[Demo](https://huggingface.co/spaces/xdecoder/Demo)]  |
| [**Pre-Trained Image Processing Transformer**]() <br> *Chen, Hanting and Wang, Yunhe and Guo, Tianyu and Xu, Chang and Deng, Yiping and Liu, Zhenhua and Ma, Siwei and Xu, Chunjing and Xu, Chao and Gao, Wen* <br> CVPR'21 <br><br> [[**Pretrained-IPT (Project)**](https://github.com/huawei-noah/Pretrained-IPT)] | ![intro](https://github.com/huawei-noah/Pretrained-IPT/raw/main/images/intro.png) | [[Github](https://github.com/huawei-noah/Pretrained-IPT)] |

## AnyX

...
